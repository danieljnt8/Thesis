{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9b1f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "device =\"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968551d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (106023, 8)\n",
      "Successfully added technical indicators\n",
      "Stock Dimension: 28, State Space: 28\n",
      "Feature Dimension: 4\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import datetime\n",
    "\n",
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "from finrl.finrl_meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "\n",
    "from finrl.finrl_meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline,convert_daily_return_to_pyfolio_ts\n",
    "\n",
    "from StockPortfolioEnv import StockPortfolioEnv as env\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
    "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
    "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
    "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
    "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
    "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)\n",
    "\n",
    "df = YahooDownloader(start_date = '2008-01-01',\n",
    "                     end_date = '2022-06-01',\n",
    "                     ticker_list = config_tickers.DOW_30_TICKER).fetch_data()\n",
    "\n",
    "fe = FeatureEngineer(\n",
    "    use_technical_indicator=True,\n",
    "    use_turbulence=False,\n",
    "    user_defined_feature=False\n",
    ")\n",
    "\n",
    "df = fe.preprocess_data(df)\n",
    "# add covariance matrix as states\n",
    "df=df.sort_values(['date','tic'],ignore_index=True)\n",
    "df.index = df.date.factorize()[0]\n",
    "cov_list = []\n",
    "return_list = []\n",
    "\n",
    "# look back is one year\n",
    "lookback=252\n",
    "for i in range(lookback,len(df.index.unique())):\n",
    "  data_lookback = df.loc[i-lookback:i,:]\n",
    "  price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n",
    "  return_lookback = price_lookback.pct_change().dropna()\n",
    "  return_list.append(return_lookback)\n",
    "\n",
    "  covs = return_lookback.cov().values \n",
    "  cov_list.append(covs)\n",
    "df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
    "df = df.merge(df_cov, on='date')\n",
    "df = df.sort_values(['date','tic']).reset_index(drop=True)\n",
    "train = data_split(df, '2009-04-01','2016-04-01')\n",
    "\n",
    "\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "tech_indicator_list = ['macd', 'rsi_30', 'cci_30', 'dx_30']\n",
    "feature_dimension = len(tech_indicator_list)\n",
    "print(f\"Feature Dimension: {feature_dimension}\")\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"transaction_cost_pct\": 0, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": tech_indicator_list, \n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-1\n",
    "    \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd8d6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_split(df, '2009-04-01','2016-06-30')\n",
    "validation = data_split(df,'2016-06-30','2019-06-30')\n",
    "test_set = data_split(df,'2019-07-01','2022-05-31')\n",
    "\n",
    "e_train_gym = env(df = train, **env_kwargs)\n",
    "e_validation_gym = env(df = validation,**env_kwargs)\n",
    "e_test_gym = env(df=test_set,**env_kwargs)\n",
    "env_train, test_obs = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59353309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        #print('Low',self.low)\n",
    "        #print('High',self.high)\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma)# * min(1.0, t / self.decay_period)\n",
    "        #print('Ou',action+ou_state)\n",
    "        #print('action clip',np.clip(action + ou_state, self.low, self.high))\n",
    "        return np.clip(action + ou_state, -1, 1)\n",
    "        #return np.clip(action,self.low,self.high)\n",
    "\n",
    "\n",
    "\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def _action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)\n",
    "        \n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2969fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 150)\n",
    "        self.linear3 = nn.Linear(150, 28)\n",
    "        self.linear4 = nn.Linear(28,1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Params state and actions are torch tensors\n",
    "        \"\"\"\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate = 1e-4):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 150)\n",
    "        self.linear3 = nn.Linear(150, output_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Param state is a torch tensor\n",
    "        \"\"\"\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.tanh(self.linear3(x)).clone()\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d2db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DDPGagent:\n",
    "    def __init__(self, state_dim,action_dim,buffer, hidden_size=400, actor_learning_rate=1e-4, critic_learning_rate=1e-4, gamma=0.99, tau=0.005):\n",
    "        # Params\n",
    "        self.num_states = state_dim\n",
    "        self.num_actions = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        # Networks\n",
    "        self.actor = Actor(self.num_states, hidden_size, self.num_actions)\n",
    "        self.actor_target = Actor(self.num_states, hidden_size, self.num_actions)\n",
    "        self.critic = Critic(self.num_states + self.num_actions, hidden_size, self.num_actions)\n",
    "        self.critic_target = Critic(self.num_states + self.num_actions, hidden_size, self.num_actions)\n",
    "\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Training\n",
    "        self.memory = buffer       \n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        action = self.actor.forward(state)\n",
    "        #print('Action_inside',action)\n",
    "        action = action.detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def get_action_clip(self,state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        action = self.actor.forward(state)\n",
    "        #print('Action_inside',action)\n",
    "        action = action.detach().numpy()[0]\n",
    "        #action = np.clip(action,0,1)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, _ = self.memory.sample(batch_size)\n",
    "        #print('Next_states',next_states)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        #print('States',states.size())\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals = self.critic.forward(states, actions)\n",
    "        next_actions = self.actor_target.forward(next_states).clamp(-1,1)\n",
    "        #print('Next_Actions',next_actions)\n",
    "        #print('Next_actions',next_actions)\n",
    "        next_Q = self.critic_target.forward(next_states, next_actions.detach())\n",
    "        #print('Rewards '+str(rewards)+' gamma '+str(self.gamma)+' next_ q' +str(next_Q))\n",
    "        \n",
    "        Qprime = rewards + self.gamma * next_Q\n",
    "        \n",
    "        #print('QVals',Qvals)\n",
    "        #print('Qprime',Qprime)\n",
    "        critic_loss = self.critic_criterion(Qvals, Qprime)\n",
    "        #critic_loss = sum(F.mse_loss(current_q, Qprime) for current_q in Qvals)\n",
    "        #print('Critic_loss',critic_loss)\n",
    "        # Actor loss\n",
    "        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean() \n",
    "        \n",
    "        #print('Policy_loss',policy_loss)\n",
    "        #print('critic Loss',critic_loss)\n",
    "        # update networks\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "       \n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "    \n",
    "    def save_model(self, path,index,timestep):\n",
    "        torch.save(self.actor.state_dict(), path+'_actor_'+str(index)+'_'+str(timestep))\n",
    "        torch.save(self.actor_target.state_dict(), path+'_target_actor_'+str(index)+'_'+str(timestep))\n",
    "        torch.save(self.critic.state_dict(), path+'_critic_'+str(index)+'_'+str(timestep))\n",
    "        torch.save(self.critic_target.state_dict(), path+'_target_critic_'+str(index)+'_'+str(timestep))\n",
    "        \n",
    "        \n",
    "\n",
    "    def load_model(self, path,index,timestep):\n",
    "        self.actor.load_state_dict(torch.load(path+'_actor_'+str(index)+'_'+str(timestep)))\n",
    "        self.actor_target.load_state_dict(torch.load(path+'_target_actor_'+str(index)+'_'+str(timestep)))\n",
    "        self.critic.load_state_dict(torch.load(path+'_critic_'+str(index)+'_'+str(timestep)))\n",
    "        self.critic_target.load_state_dict(torch.load(path+'_target_critic_'+str(index)+'_'+str(timestep)))\n",
    "        \n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077021d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_action(action):\n",
    "    low  = 0\n",
    "    high = 1\n",
    "    action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "    action = np.clip(action, low, high)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2c5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_ensemble(number_of_agents, seed,env_train):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    k = number_of_agents\n",
    "    Buffer = Memory(max_size = 1000000)\n",
    "    e_train = NormalizedEnv(e_train_gym)\n",
    "\n",
    "    env_train,test_obs = e_train.get_sb_env()\n",
    "    agents = list()\n",
    "    for i in range(k):\n",
    "        agents.append(DDPGagent(state_dim = 896,action_dim=28,buffer = Buffer ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    noise = OUNoise(env_train.action_space)\n",
    "    batch_size = 256\n",
    "    rewards = []\n",
    "    avg_rewards = []\n",
    "    env_train.reset()\n",
    "    noise.reset()\n",
    "    path ='MultiDDPG/17Aug/'\n",
    "    \n",
    "    \n",
    "    #from DDPG import DDPGagent\n",
    "    #from utils import *\n",
    "\n",
    "    #numer of agents \n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(100000):\n",
    "\n",
    "\n",
    "        obs_tensor = th.tensor(test_obs,requires_grad=False)\n",
    "        state =th.flatten(obs_tensor).cpu().detach().numpy()\n",
    "\n",
    "        index_agent = episode % k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        action = agents[index_agent].get_action(state)\n",
    "        #print('Action_1',action)\n",
    "\n",
    "\n",
    "        action = noise.get_action(action)\n",
    "        action_normalized = normalized_action(action)\n",
    "\n",
    "        #print('Daniel')\n",
    "        test_obs, reward, done, _ = env_train.step(np.array([action_normalized]))\n",
    "        #print('Action',action)\n",
    "        #print('Reward',reward)\n",
    "\n",
    "        new_obs_tensor = th.tensor(test_obs,requires_grad=False)\n",
    "        new_state =th.flatten(new_obs_tensor).cpu().detach().numpy()\n",
    "        Buffer.push(state, action, reward, new_state, done)\n",
    "        rewards.append(reward)\n",
    "        if len(Buffer) > batch_size and episode % 1024 == 0 and episode>10000:\n",
    "            print('Updating')\n",
    "            for _ in range(100) : \n",
    "\n",
    "                for agent in agents : \n",
    "                    #print('Updating')\n",
    "                    agent.update(batch_size)   \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if done[0]:\n",
    "            print('Hier')\n",
    "            env_train,test_obs = e_train.get_sb_env()\n",
    "            env_train.reset()\n",
    "\n",
    "        \n",
    "    return agents,rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0bd8df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-djonatan/anaconda3/envs/empirical/lib/python3.7/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:3409521.883116278\n",
      "Sharpe:  1.1506013963228752\n",
      "=================================\n",
      "Hier\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4255/2270841108.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magents1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me_train_gym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magents2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31202\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me_train_gym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magents3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me_train_gym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magents4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m893\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me_train_gym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0magents5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m98927\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me_train_gym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4255/177485231.py\u001b[0m in \u001b[0;36mtrain_ensemble\u001b[0;34m(number_of_agents, seed, env_train)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#print('Daniel')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mtest_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_normalized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;31m#print('Action',action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m#print('Reward',reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/empirical/lib/python3.7/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/empirical/lib/python3.7/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/export/homebrick/home/ml-djonatan/empirical/StockPortfolioEnv.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    154\u001b[0m                                    [self.data[tech].values.tolist() for tech in self.tech_indicator_list], axis=0)\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mportfolio_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlast_day_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mlog_portfolio_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlast_day_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# update portfolio value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agents1,rewards1 = train_ensemble(4,150,e_train_gym)\n",
    "agents2,rewards2 = train_ensemble(4,31202,e_train_gym)\n",
    "agents3,rewards3 = train_ensemble(4,42,e_train_gym)\n",
    "agents4,rewards4 = train_ensemble(4,893,e_train_gym)\n",
    "agents5,rewards5 = train_ensemble(4,98927,e_train_gym)\n",
    "agents6,rewards6 = train_ensemble(4,2912,e_train_gym)\n",
    "agents7,rewards7 = train_ensemble(4,8723,e_train_gym)\n",
    "agents8,rewards8 = train_ensemble(4,1233,e_train_gym)\n",
    "agents9,rewards9 = train_ensemble(4,596,e_train_gym)\n",
    "agents10,rewards10 = train_ensemble(4,1,e_train_gym)\n",
    "agents11,rewards11 = train_ensemble(4,10,e_train_gym)\n",
    "agents12,rewards12= train_ensemble(4,100,e_train_gym)\n",
    "agents13,rewards13= train_ensemble(4,2,e_train_gym)\n",
    "agents14,rewards14= train_ensemble(4,20,e_train_gym)\n",
    "agents15,rewards15 = train_ensemble(4,200,e_train_gym)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,env,seed):\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    test_env, test_obs = env.get_sb_env()\n",
    "    \n",
    "\n",
    "    account_memory = []\n",
    "    actions_memory = []\n",
    "    #         state_memory=[] #add memory pool to store states\n",
    "    test_env.reset()\n",
    "    for i in range(len(env.df.index.unique())):\n",
    "    \n",
    "        \n",
    "        obs_tensor = th.tensor(test_obs,requires_grad=False)\n",
    "        state =th.flatten(obs_tensor).cpu().detach().numpy()\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        action = agent.get_action_clip(state)\n",
    "        action = normalized_action(action)\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        test_obs, reward, done, _ = test_env.step(np.array([action]))\n",
    "        \n",
    "        \n",
    "    \n",
    "        if i == (len(env.df.index.unique()) - 2):\n",
    "            print('hier')\n",
    "\n",
    "            account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "            actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "    #                 state_memory=test_env.env_method(method_name=\"save_state_memory\") # add current state to state memory\n",
    "        if done[0]:\n",
    "            print(\"hit end!\")\n",
    "            break\n",
    "    return account_memory,actions_memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
