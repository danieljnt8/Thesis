{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import datetime\n",
    "\n",
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "from finrl.finrl_meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "\n",
    "from finrl.finrl_meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline,convert_daily_return_to_pyfolio_ts\n",
    "\n",
    "from Environment import StockPortfolioEnv as env\n",
    "import sys\n",
    "\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
    "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
    "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
    "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
    "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
    "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)\n",
    "\n",
    "df = YahooDownloader(start_date = '2008-01-01',\n",
    "                     end_date = '2022-06-01',\n",
    "                     ticker_list = config_tickers.DOW_30_TICKER).fetch_data()\n",
    "\n",
    "fe = FeatureEngineer(\n",
    "    use_technical_indicator=True,\n",
    "    use_turbulence=False,\n",
    "    user_defined_feature=False\n",
    ")\n",
    "\n",
    "df = fe.preprocess_data(df)\n",
    "# add covariance matrix as states\n",
    "df=df.sort_values(['date','tic'],ignore_index=True)\n",
    "df.index = df.date.factorize()[0]\n",
    "cov_list = []\n",
    "return_list = []\n",
    "\n",
    "# look back is one year\n",
    "lookback=252\n",
    "for i in range(lookback,len(df.index.unique())):\n",
    "  data_lookback = df.loc[i-lookback:i,:]\n",
    "  price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n",
    "  return_lookback = price_lookback.pct_change().dropna()\n",
    "  return_list.append(return_lookback)\n",
    "\n",
    "  covs = return_lookback.cov().values \n",
    "  cov_list.append(covs)\n",
    "df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
    "df = df.merge(df_cov, on='date')\n",
    "df = df.sort_values(['date','tic']).reset_index(drop=True)\n",
    "train = data_split(df, '2009-04-01','2016-04-01')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "tech_indicator_list = ['macd', 'rsi_30', 'cci_30', 'dx_30']\n",
    "feature_dimension = len(tech_indicator_list)\n",
    "print(f\"Feature Dimension: {feature_dimension}\")\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"transaction_cost_pct\": 0, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": tech_indicator_list, \n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-1\n",
    "    \n",
    "}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = data_split(df, '2009-04-01','2016-06-30')\n",
    "validation = data_split(df,'2016-06-30','2019-06-30')\n",
    "test_set = data_split(df,'2019-07-01','2022-06-31')\n",
    "\n",
    "e_train_gym = env(df = train, **env_kwargs)\n",
    "e_validation_gym = env(df = validation,**env_kwargs)\n",
    "e_test_gym = env(df=test_set,**env_kwargs)\n",
    "env_train, test_obs = e_train_gym.get_sb_env()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C,PPO,DDPG\n",
    "\n",
    "\n",
    "trained_a2c_1 = A2C.load('trained_models/5_Oct/a2c_1')\n",
    "trained_ppo_1 = PPO.load('trained_models/5_Oct/ppo_2')\n",
    "trained_ddpg_1 = DDPG.load('trained_models/5_Oct/ddpg_1')\n",
    "\n",
    "trained_a2c_2 = A2C.load('trained_models/5_Oct/a2c_2')\n",
    "trained_ppo_2 = PPO.load('trained_models/5_Oct/ppo_2')\n",
    "trained_ddpg_2 = DDPG.load('trained_models/5_Oct/ddpg_2')\n",
    "\n",
    "\n",
    "\n",
    "trained_a2c_3 = A2C.load('trained_models/5_Oct/a2c_3')\n",
    "trained_ppo_3 = PPO.load('trained_models/5_Oct/ppo_3')\n",
    "trained_ddpg_3 = DDPG.load('trained_models/5_Oct/ddpg_3')\n",
    "\n",
    "trained_a2c_4 = A2C.load('trained_models/5_Oct/a2c_4')\n",
    "trained_ppo_4 = PPO.load('trained_models/5_Oct/ppo_4')\n",
    "trained_ddpg_4 = DDPG.load('trained_models/5_Oct/ddpg_4')\n",
    "\n",
    "trained_a2c_5 = A2C.load('trained_models/5_Oct/a2c_5')\n",
    "trained_ppo_5 = PPO.load('trained_models/5_Oct/ppo_5')\n",
    "trained_ddpg_5 = DDPG.load('trained_models/5_Oct/ddpg_5')\n",
    "\n",
    "trained_a2c_6 = A2C.load('trained_models/5_Oct/a2c_6')\n",
    "trained_ppo_6 = PPO.load('trained_models/5_Oct/ppo_6')\n",
    "trained_ddpg_6 = DDPG.load('trained_models/5_Oct/ddpg_6')\n",
    "\n",
    "\n",
    "trained_a2c_7= A2C.load('trained_models/5_Oct/a2c_7')\n",
    "trained_ppo_7 = PPO.load('trained_models/5_Oct/ppo_7')\n",
    "trained_ddpg_7 = DDPG.load('trained_models/5_Oct/ddpg_7')\n",
    "\n",
    "trained_a2c_8 = A2C.load('trained_models/5_Oct/a2c_8')\n",
    "trained_ppo_8 = PPO.load('trained_models/5_Oct/ppo_8')\n",
    "trained_ddpg_8 = DDPG.load('trained_models/5_Oct/ddpg_8')\n",
    "\n",
    "trained_a2c_9 = A2C.load('trained_models/5_Oct/a2c_9')\n",
    "trained_ppo_9 = PPO.load('trained_models/5_Oct/ppo_9')\n",
    "trained_ddpg_9 = DDPG.load('trained_models/5_Oct/ddpg_9')\n",
    "\n",
    "\n",
    "trained_a2c_10= A2C.load('trained_models/5_Oct/a2c_seed_10')\n",
    "trained_ppo_10 = PPO.load('trained_models/5_Oct/ppo_10')\n",
    "trained_ddpg_10 = DDPG.load('trained_models/5_Oct/ddpg_10')\n",
    "pool_agent_1 = [trained_a2c_1,trained_ppo_1,trained_ddpg_1]\n",
    "pool_agent_2 = [trained_a2c_2,trained_ppo_2,trained_ddpg_2]\n",
    "pool_agent_3 = [trained_a2c_3,trained_ppo_3,trained_ddpg_3]\n",
    "pool_agent_4 = [trained_a2c_4,trained_ppo_4,trained_ddpg_4]\n",
    "pool_agent_5 = [trained_a2c_5,trained_ppo_5,trained_ddpg_5]\n",
    "pool_agent_6 = [trained_a2c_6,trained_ppo_6,trained_ddpg_6]\n",
    "pool_agent_7 = [trained_a2c_7,trained_ppo_7,trained_ddpg_7]\n",
    "pool_agent_8 = [trained_a2c_8,trained_ppo_8,trained_ddpg_8]\n",
    "pool_agent_9 = [trained_a2c_9,trained_ppo_9,trained_ddpg_9]\n",
    "pool_agent_10 = [trained_a2c_10,trained_ppo_10,trained_ddpg_10]\n",
    "\n",
    "pool_agents = [pool_agent_1,pool_agent_2,pool_agent_4,pool_agent_5,pool_agent_6,pool_agent_7,pool_agent_8,\n",
    "              pool_agent_9,pool_agent_10,pool_agent_3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trained_a2c = [trained_a2c_1,trained_a2c_2,trained_a2c_4,trained_a2c_3,trained_a2c_5,\n",
    "              trained_a2c_6,trained_a2c_7,trained_a2c_8,trained_a2c_9,trained_a2c_10]\n",
    "              \n",
    "\n",
    "trained_ppo = [trained_ppo_1,trained_ppo_2,trained_ppo_4,trained_ppo_3,trained_ppo_5,\n",
    "              trained_ppo_6,trained_ppo_7,trained_ppo_8,trained_ppo_9,trained_ppo_10]\n",
    "              \n",
    "trained_ddpg =[trained_ddpg_1,trained_ddpg_2,trained_ddpg_3,trained_ddpg_4,trained_ddpg_5,\n",
    "              trained_ddpg_6,trained_ddpg_7,trained_ddpg_8,trained_ddpg_9,trained_ddpg_10]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_single(trained_agent,env):\n",
    "    test_env, test_obs = env.get_sb_env()\n",
    "\n",
    "    account_memory = []\n",
    "    actions_memory = []\n",
    "    #         state_memory=[] #add memory pool to store states\n",
    "    test_env.reset()\n",
    "    for i in range(len(env.df.index.unique())):\n",
    "        obs_tensor=th.tensor(test_obs)\n",
    "        #print(trained_agent.policy._predict(obs_tensor,deterministic=True))\n",
    "        action_a2c_ensemble, _states_a2c_ensemble = trained_agent.predict(test_obs, deterministic=True)\n",
    "        #print(action_a2c_ensemble)\n",
    "        test_obs, rewards, dones, info = test_env.step(np.array(action_a2c_ensemble))\n",
    "        #print(i)\n",
    "        if i == (len(env.df.index.unique()) - 2):\n",
    "\n",
    "            account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "            actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "    #                 state_memory=test_env.env_method(method_name=\"save_state_memory\") # add current state to state memory\n",
    "        if dones[0]:\n",
    "            print(\"hit end!\")\n",
    "            break\n",
    "    return account_memory,actions_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e61aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCenter(actions):\n",
    "    while len(actions) >2 :\n",
    "            mean = np.mean(actions,axis=0)\n",
    "\n",
    "            distance = list(range(len(actions)))\n",
    "            for k in range(0,len(distance)) :\n",
    "                distance[k] = np.linalg.norm(mean-actions[k])\n",
    "\n",
    "            #print(distance)\n",
    "            actions.pop(distance.index(max(distance)))\n",
    "\n",
    "    action = np.mean(actions,axis=0)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebd37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "def test_ensemble(ensemble_type,env,trained_agents):\n",
    "    test_env,test_obs = env.get_sb_env()\n",
    "    \n",
    "    account_memory = []\n",
    "    actions_memory = []\n",
    "    \n",
    "    a2c_agent = trained_agents[0]\n",
    "    ppo_agent = trained_agents[1]\n",
    "    ddpg_agent = trained_agents[2]\n",
    "    \n",
    "    for s in range(len(env.df.index.unique())):\n",
    "        obs_tensor=th.tensor(test_obs,requires_grad=False)#.cuda()\n",
    "        obs_flatten =th.flatten(obs_tensor).cpu().detach().numpy()\n",
    "        \n",
    "        a2c_action = a2c_agent.predict(test_obs,deterministic=True)[0]\n",
    "        ppo_action = ppo_agent.predict(test_obs,deterministic=True)[0]\n",
    "        ddpg_action = ddpg_agent.predict(test_obs,deterministic=True)[0]\n",
    "        \n",
    "        join_prediction=np.concatenate((np.concatenate((a2c_action,ppo_action)),ddpg_action))\n",
    "        #print(join_prediction)\n",
    "        \n",
    "        if ensemble_type=='dataCenter':\n",
    "            action = dataCenter(join_prediction.tolist())\n",
    "        \n",
    "        elif ensemble_type == 'densityBased' :\n",
    "            \n",
    "            density_prediction = [calculate_density(p,join_prediction,1) for p in join_prediction]\n",
    "            action = join_prediction[np.argmax(density_prediction)].tolist()\n",
    "        \n",
    "        else:\n",
    "            action = np.mean(join_prediction,axis=0)\n",
    "        #action = np.mean(a2c_array,axis=0)\n",
    "        \n",
    "        test_obs, rewards, dones, info = test_env.step(np.array([action]))\n",
    "        #print(i)\n",
    "        \n",
    "        if s == (len(env.df.index.unique()) - 2):\n",
    "\n",
    "            account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "            actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "    #                 state_memory=test_env.env_method(method_name=\"save_state_memory\") # add current state to state memory\n",
    "        if dones[0]:\n",
    "            print(\"hit end!\")\n",
    "            break\n",
    "    return account_memory,actions_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db99506",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ensemble('dataCenter',e_test_gym,pool_agents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04006ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e5a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}